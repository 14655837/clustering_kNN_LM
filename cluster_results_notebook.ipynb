{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5abd89",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31fcec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import joblib\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hdbscan\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94042b4",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb288f",
   "metadata": {},
   "source": [
    "This would look different for different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22befc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"validation_it/validation_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d55d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = joblib.load(\"validation_it/rest_keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465e7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = joblib.load(\"validation_it/rest__ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2ecb8",
   "metadata": {},
   "source": [
    "For clustering, first PCA is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36cd55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_keys = PCA(n_components = 20).fit(keys[:50_000]).transform(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary it can be save\n",
    "joblib.dump(pca_keys, \"pca_keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "630d6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_keys = joblib.load(\"it_keys_pca_to_20.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbfe46",
   "metadata": {},
   "source": [
    "## Clustering with the datastore as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_labels_to_keys_and_ids(labels, keys, ids):\n",
    "\n",
    "    all_different_ids = np.unique(ids)\n",
    "\n",
    "    new_keys = []\n",
    "    new_ids = []\n",
    "\n",
    "    for label in all_different_ids:\n",
    "        i = np.where(labels == label)[0]\n",
    "        if len(i) > 0:\n",
    "            new_keys.append(np.mean(keys[i], axis=0))\n",
    "            \n",
    "            # Get the most frequent ID\n",
    "            counts = np.bincount(ids[i])\n",
    "            most_freq_id = np.argmax(counts)\n",
    "            new_ids.append(most_freq_id)\n",
    "\n",
    "    return new_keys, new_ids\n",
    "\n",
    "# if a clusteringm ethod returns clusters of -1 for items that are not part of a cluster, like HDBSCAN and DBSCAN do use the following\n",
    "\n",
    "def from_labels_to_keys_and_ids_with_minus_1(labels, keys, ids):\n",
    "\n",
    "    all_different_ids = np.unique(ids)\n",
    "\n",
    "    new_keys = []\n",
    "    new_ids = []\n",
    "\n",
    "    for label in all_different_ids[1:]:\n",
    "        i = np.where(labels == label)[0]\n",
    "        if len(i) > 0:\n",
    "            new_keys.append(np.mean(keys[i], axis=0))\n",
    "            \n",
    "            # Get the most frequent ID\n",
    "            counts = np.bincount(ids[i])\n",
    "            most_freq_id = np.argmax(counts)\n",
    "            new_ids.append(most_freq_id)\n",
    "\n",
    "    i = np.where(labels == -1)\n",
    "    for item in ids[i]:\n",
    "        new_ids.append(item)\n",
    "\n",
    "    for item in keys[i]:\n",
    "        new_keys.append(item)\n",
    "\n",
    "    return new_keys, new_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fb9e3",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f989c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdbscan_cluster(X):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')\n",
    "    return clusterer.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2648a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_labels = hdbscan_cluster(pca_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec0d62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6839"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_keys, new_ids = from_labels_to_keys_and_ids_with_minus_1(hdbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f34f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Validation/validation_results/hdbscan_labels_commonvoice.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(new_keys, \"name\")\n",
    "joblib.dump(new_ids, \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If necessary to save memory\n",
    "hdbscan_labels = None\n",
    "new_keys = None\n",
    "new_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f1c29",
   "metadata": {},
   "source": [
    "## Minibatch kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31ee4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_large_centroid_pool(X, target_centroids=1_200_000, reps = 50):\n",
    "    all_centroids = []\n",
    "    n_clusters_per_iter = target_centroids // reps\n",
    "\n",
    "    for i in range(reps):\n",
    "        mbk = MiniBatchKMeans(\n",
    "            n_clusters=n_clusters_per_iter,\n",
    "            batch_size = 512,\n",
    "            n_init=1,\n",
    "            random_state=i  # Different seed each time\n",
    "            max_iter = 10\n",
    "        )\n",
    "        mbk.fit(X)\n",
    "        all_centroids.append(mbk.cluster_centers_)\n",
    "\n",
    "    return np.vstack(all_centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdd2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def massive_clustering(X, final_clusters=1_200_000, chunk_size=100_000, local_clusters=1000):\n",
    "    all_centroids = generate_large_centroid_pool(X)\n",
    "    print(len(all_centroids))\n",
    "\n",
    "    mbk_final = MiniBatchKMeans(n_clusters=final_clusters, batch_size=1000, random_state=42)\n",
    "    final_labels = mbk_final.fit_predict(all_centroids)\n",
    "\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d0baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "massive_mbk_labels = massive_clustering(pca_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys, new_ids = from_labels_to_keys_and_ids(massive_mbk_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c145599",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(new_keys, \"name\")\n",
    "joblib.dump(new_ids, \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If necessary to save memory\n",
    "hdbscan_labels = None\n",
    "new_keys = None\n",
    "new_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8537b9",
   "metadata": {},
   "source": [
    "k-Means (with the faiss library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3f11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(len(pca_keys) / 2)\n",
    "index = faiss.IndexFlatL2(pca_keys.shape[1])\n",
    "kmeans = faiss.Clustering(pca_keys.shape[1], k)\n",
    "kmeans.train(pca_keys, index)\n",
    "_, labels = index.search(pca_keys, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys, new_ids = from_labels_to_keys_and_ids(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21f39c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mjoblib\u001b[49m.dump(new_keys, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m joblib.dump(new_ids, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "joblib.dump(new_keys, \"name\")\n",
    "joblib.dump(new_ids, \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d91d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If necessary to save memory\n",
    "hdbscan_labels = None\n",
    "new_keys = None\n",
    "new_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03608b7a",
   "metadata": {},
   "source": [
    "# Clustering only the same id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e31d698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keys_and_ids_from_labels(indices_of_labels, labels):\n",
    "    dict_keys = list(labels.keys())\n",
    "    dict_keys[:5], len(dict_keys)\n",
    "\n",
    "    new_vectors = []\n",
    "    new_ids = []\n",
    "    \n",
    "    for id in dict_keys:\n",
    "        temporary_labels = labels[id]\n",
    "        temporary_indexes = indices_of_labels[id]\n",
    "        temp_keys = keys[temporary_indexes]\n",
    "        for label in np.unique(temporary_labels):\n",
    "            new_vectors.append(np.mean(temp_keys[np.where(temporary_labels == label)], 0))\n",
    "            new_ids.append(id)\n",
    "\n",
    "    clustered_indices = [item for key in labels.keys() for item in indices_of_labels[key]]\n",
    "\n",
    "    all_indices = np.arange(len(keys))\n",
    "    non_clustered_indices = np.setdiff1d(all_indices, clustered_indices)\n",
    "\n",
    "    non_clustered_keys = list(keys[non_clustered_indices])\n",
    "    non_clustered_ids = list(ids[non_clustered_indices])\n",
    "\n",
    "    all_new_keys = non_clustered_keys + new_vectors\n",
    "    all_new_ids = non_clustered_ids + new_ids\n",
    "\n",
    "    #if wanted:\n",
    "    all_new_keys = np.array(all_new_keys)\n",
    "    all_new_ids = np.array(all_new_ids)\n",
    "\n",
    "    return all_new_keys, all_new_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44cda1",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_cluster(X):\n",
    "    dbscan = DBSCAN(eps = 1.5, min_samples=1)\n",
    "    return dbscan.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_different_ids = np.unique(ids)\n",
    "\n",
    "indices_of_labels = dict()\n",
    "labels = dict()\n",
    "\n",
    "for id in all_different_ids:\n",
    "    #print(id)\n",
    "    indices = np.where(ids == id)[0]\n",
    "    sub = np.array(pca_keys[indices]).astype(np.float32)\n",
    "    if len(sub) > 50:\n",
    "        labels[id] = dbscan_cluster(sub)\n",
    "        print(len(labels[id]), len(np.unique(labels[id])))\n",
    "        indices_of_labels[id] = indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40439a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys, new_ids = save_keys_and_ids_from_labels(indices_of_labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(new_keys, \"name\")\n",
    "joblib.dump(new_ids, \"name\")\n",
    "#If necessary to save memory\n",
    "hdbscan_labels = None\n",
    "new_keys = None\n",
    "new_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c89f39",
   "metadata": {},
   "source": [
    "### Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8710c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 6, 7, 8]), 5584)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_different_ids = np.unique(ids)\n",
    "all_different_ids[:5], len(all_different_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3665d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def birch_cluster(X):\n",
    "    length = len(X)\n",
    "    clusterer = Birch(threshold=0.3, branching_factor=25, n_clusters= min(20, int(length / 4)))\n",
    "    return clusterer.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314acc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def birch_cluster_in_steps(X, batch_size=50):\n",
    "    X = X.astype(np.float32)\n",
    "    length = len(X)\n",
    "    clusterer = Birch(threshold = 1, branching_factor=25, n_clusters=None)\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch = X[i:i + batch_size]\n",
    "        clusterer.partial_fit(batch)\n",
    "\n",
    "    return clusterer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_labels = dict()\n",
    "labels = dict()\n",
    "\n",
    "for id in all_different_ids:\n",
    "    #print(id)\n",
    "    indices = np.where(ids == id)[0]\n",
    "    sub = np.array(pca_keys[indices]).astype(np.float32)\n",
    "    if len(sub) > 50:\n",
    "        labels[id] = birch_cluster_in_steps(sub)\n",
    "        print(len(labels[id]), len(np.unique(labels[id])))\n",
    "        indices_of_labels[id] = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c23cb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys, new_ids = save_keys_and_ids_from_labels(indices_of_labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ce31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(new_keys, \"name\")\n",
    "joblib.dump(new_ids, \"name\")\n",
    "#If necessary to save memory\n",
    "hdbscan_labels = None\n",
    "new_keys = None\n",
    "new_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beef36b",
   "metadata": {},
   "source": [
    "### Mini Batch K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc7905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_minibatchkmeans(X):\n",
    "    data_size = len(X)\n",
    "    #Batch size for smaller dataset was batch_size = int(data_size**0.4), no reassignment ration. max_no_imp was 10\n",
    "    clusterer = MiniBatchKMeans(init ='k-means++', n_clusters = int(data_size / 3),\n",
    "                        batch_size = min(1000, int(data_size**0.25)), n_init = 1,\n",
    "                        max_no_improvement = 5, verbose = 0, reassignment_ratio=0.01)\n",
    "    return clusterer.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b934077",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_labels = dict()\n",
    "labels = dict()\n",
    "\n",
    "for id in all_different_ids:\n",
    "    indices = np.where(ids == id)[0]\n",
    "    sub = np.array(pca_keys[indices])\n",
    "    if len(sub) > 50:\n",
    "        print(id)\n",
    "        labels[id] = cluster_minibatchkmeans(sub)\n",
    "        indices_of_labels[id] = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys, new_ids = save_keys_and_ids_from_labels(indices_of_labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe74b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(new_keys, \"name\")\n",
    "joblib.dump(new_ids, \"name\")\n",
    "#If necessary to save memory\n",
    "hdbscan_labels = None\n",
    "new_keys = None\n",
    "new_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05483d",
   "metadata": {},
   "source": [
    "### Hdhbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f77a5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdbscan_cluster(X):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')\n",
    "    return clusterer.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_labels = dict()\n",
    "labels = dict()\n",
    "\n",
    "for id in all_different_ids:\n",
    "    indices = np.where(ids == id)[0]\n",
    "    sub = np.array(pca_keys[indices])\n",
    "    if len(sub) > 50:\n",
    "        print(id)\n",
    "        labels_hdbscan = hdbscan_cluster(sub)\n",
    "        return_i = [i for i in range(len(labels_hdbscan)) if labels_hdbscan[i] != -1]\n",
    "        return_labels = [item for item in labels_hdbscan if item != -1]\n",
    "        labels[id] = return_labels\n",
    "        indices_of_labels[id] = indices[return_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys, new_ids = save_keys_and_ids_from_labels(indices_of_labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ca6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(new_keys, \"name\")\n",
    "joblib.dump(new_ids, \"name\")\n",
    "#If necessary to save memory\n",
    "hdbscan_labels = None\n",
    "new_keys = None\n",
    "new_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2242e6ee",
   "metadata": {},
   "source": [
    "### AgglomerativeClustering / Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Agglomerative_Clustering(X):\n",
    "    length = len(X)\n",
    "    n_clusters = int(length / 3)\n",
    "    agg = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    return agg.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d70b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_labels = dict()\n",
    "labels = dict()\n",
    "\n",
    "for id in all_different_ids:\n",
    "    indices = np.where(ids == id)[0]\n",
    "    sub = np.array(pca_keys[indices])\n",
    "    if len(sub) > 50:\n",
    "        print(id)\n",
    "        labels[id] = AgglomerativeClustering(sub)\n",
    "        indices_of_labels[id] = indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys, new_ids = save_keys_and_ids_from_labels(indices_of_labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2614a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(new_keys, \"name\")\n",
    "joblib.dump(new_ids, \"name\")\n",
    "#If necessary to save memory\n",
    "hdbscan_labels = None\n",
    "new_keys = None\n",
    "new_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2aac17",
   "metadata": {},
   "source": [
    "### Randomly removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_delete = int(0.333 * index.ntotal)\n",
    "total_in_index = index.ntotal\n",
    "indices = np.array(random.sample(range(total_in_index), num_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7504a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys = keys[indices]\n",
    "new_ids = ids[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(new_keys, \"name\")\n",
    "joblib.dump(new_ids, \"name\")\n",
    "#If necessary to save memory\n",
    "hdbscan_labels = None\n",
    "new_keys = None\n",
    "new_ids = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
